# Spec Research: Workflow Handoffs

## Summary

This research examines how to programmatically chain PAW workflow stages through automated agent handoffs with varying levels of automation. The VS Code Language Model API provides commands for creating new chat sessions (`workbench.action.chat.newChat`) and invoking agents (`workbench.action.chat.open` with `mode: 'agent'`), though invocation is fire-and-forget with no programmatic access to agent responses. Context passing between agents relies on the `paw_get_context` tool, which agents call with a Work ID to retrieve workspace instructions, user instructions, and WorkflowContext.md content.

The research identifies behavioral patterns for optimal automation: standard workflows benefit from automatic Spec → Research → Spec transitions with pauses before Implementation, while review workflows can automate Understanding → Research cycles. The Status Agent can efficiently scan `.paw/work/` by listing top-level directories without recursing into artifacts, checking for WorkflowContext.md presence to identify active workflows. Git divergence detection uses standard commands (`git rev-list`, `git status`), while command keywords follow existing PAW terminology (code-formatted with backticks in agent instructions).

Stage prerequisite validation checks for required artifact files (ImplementationPlan.md before implementing), while prompt file naming uses stage-specific patterns (`02A-code-research.prompt.md`) already established. Agent determination from frontmatter reads the `agent:` field, with fallback to filename parsing. The GitHub MCP Server provides PR querying methods, though specific batching capabilities and rate limits require external validation.

## Research Findings

### Question 1: VS Code Language Model API for Chat Sessions and Agent Invocation

**Question**: What are the exact VS Code Language Model API methods for creating new chat sessions and invoking agents programmatically? Document the complete API signature, required parameters, and example usage.

**Answer**: 

The VS Code Language Model API provides two commands for chat management:

1. **Creating new chat sessions**: `workbench.action.chat.newChat`
   - No parameters required
   - Returns a Promise (value is logged but not used programmatically)
   - Opens a fresh chat panel with empty context

2. **Invoking agents in chat**: `workbench.action.chat.open`
   - Required parameter: `query` (string) - The complete prompt text with instructions
   - Required parameter: `mode` (string) - Must be `'agent'` to invoke agent mode vs conversational chat
   - Returns: `Thenable<void>` - Resolves when chat panel opens, does NOT wait for agent completion
   - Agent execution is asynchronous and independent

**API Signature**:
```typescript
vscode.commands.executeCommand(
  'workbench.action.chat.open',
  {
    query: string,      // Complete prompt with all context and instructions
    mode: 'agent'       // Required for agent mode invocation
  }
): Thenable<void>
```

**Current Usage Pattern** (from `src/commands/initializeWorkItem.ts`):
```typescript
await vscode.commands.executeCommand('workbench.action.chat.newChat')
  .then(async value => {
    outputChannel.appendLine('[INFO] New chat session created: ' + String(value));
    await vscode.commands.executeCommand('workbench.action.chat.open', {
      query: prompt,
      mode: 'agent'
    });
  });
```

**Limitations**:
- Extension cannot wait for agent to complete task
- No programmatic access to agent output or success/failure status
- Cannot receive agent response in code
- Cannot verify agent created files or performed actions directly

The pattern is fire-and-forget: extension constructs comprehensive prompt, opens chat with agent context, and relies on user to monitor agent execution in chat panel.

**Evidence**: Implementation in `src/commands/initializeWorkItem.ts:105-115`; prior research in `.paw/work/vscode-extension-init/SpecResearch.md:833-920`

**Implications**: Handoff tool will follow same pattern—construct prompt with Work ID and instructions, invoke `workbench.action.chat.open` with `mode: 'agent'`, target agent then calls `paw_get_context` to retrieve full context. No ability to programmatically chain agents directly; each handoff creates new chat and relies on tool approval flow for user control.

---

### Question 2: VS Code Tool Approval Mechanism

**Question**: How does VS Code's tool approval mechanism work for Language Model Tools? What configuration options exist for auto-approval? What happens when tool approval times out?

**Answer**: 

VS Code Language Model Tools require user approval before invocation through a confirmation UI. The mechanism operates at the tool registration level:

**Approval Flow**:
1. Agent invokes tool during conversation
2. VS Code presents confirmation UI generated by tool's `prepareInvocation()` method
3. User sees:
   - `invocationMessage`: Brief description of what will happen (e.g., "Retrieving PAW context for feature: auth-system")
   - `confirmationMessages`: Title and detailed markdown body explaining the operation
4. User chooses:
   - Approve once (single execution)
   - Always Allow (auto-approve future invocations of this tool)
   - Deny (tool invocation fails)

**Configuration in Tool Implementation** (from `src/tools/contextTool.ts:313-327`):
```typescript
const tool = vscode.lm.registerTool<ContextParams>(
  'paw_get_context',
  {
    async prepareInvocation(options, _token) {
      const { feature_slug, agent_name } = options.input;
      return {
        invocationMessage: `Retrieving PAW context for feature: ${feature_slug} (agent: ${agent_name})`,
        confirmationMessages: {
          title: 'Get PAW Context',
          message: new vscode.MarkdownString(
            `This will retrieve custom instructions and workflow context for:\n\n` +
            `- **Feature**: ${feature_slug}\n` +
            `- **Agent**: ${agent_name}`
          )
        }
      };
    },
    // ... invoke method
  }
);
```

**Auto-Approval**:
- Controlled by user selecting "Always Allow" in the approval UI
- Per-tool setting (user can auto-approve `paw_get_context` but still review `paw_create_prompt_templates`)
- No programmatic API to set auto-approval from extension code
- Settings persist across VS Code sessions

**Timeout Behavior**:
- Timeout is controlled by VS Code/GitHub Copilot, not the extension
- Extension respects timeout without custom handling
- When timeout occurs, tool invocation is treated as denied
- Agent receives error indicating tool approval failed or timed out
- Extension cannot programmatically detect or handle timeout differently from denial

**Evidence**: Tool registration pattern in `src/tools/contextTool.ts:310-365` and `src/tools/createPromptTemplates.ts:347-387`; confirmation UI implementation; prior research noting extension cannot customize timeout behavior

**Implications**: For Auto mode handoffs, users will need to set "Always Allow" for the handoff tool to enable continuous agent chaining. Semi-Auto and Manual modes will show approval prompts at each handoff. Timeout handling is beyond extension control; clear error messages in agents will guide users if approval times out.

---

### Question 3: Context Passing Between Agent Invocations

**Question**: What is the best practice for passing context between agent invocations when creating new chat sessions? Should context be embedded in the prompt string, passed via tool parameters, or retrieved dynamically by the target agent?

**Answer**: 

PAW uses **dynamic context retrieval** by target agents rather than embedding full context in prompts. This pattern is established and implemented through the `paw_get_context` tool.

**Current Pattern**:
1. Handoff prompt contains **minimal parameters only**: Work ID (feature slug)
2. Target agent starts with fresh chat session (no inherited context)
3. Agent's first action: call `paw_get_context` tool with Work ID and agent name
4. Tool returns:
   - Workspace-specific custom instructions (`.paw/instructions/<agent>-instructions.md`)
   - User-level custom instructions (`~/.paw/instructions/<agent>-instructions.md`)
   - Raw WorkflowContext.md content (`.paw/work/<work-id>/WorkflowContext.md`)
5. Agent parses WorkflowContext.md to extract parameters (Target Branch, Issue URL, Review Strategy, etc.)
6. Agent reads required artifacts from disk based on stage (Spec.md, ImplementationPlan.md, etc.)

**Prompt Template Pattern** (from `src/tools/createPromptTemplates.ts:230-250`):
```typescript
function generatePromptTemplate(
  mode: string,
  instruction: string,
  featureSlug: string
): string {
  return `---\nagent: ${mode}\n---\n\n${instruction}\n\nWork ID: ${featureSlug}\n`;
}
```

Generated prompt files contain only:
- Frontmatter: `agent: PAW-02A Code Researcher`
- Instruction: "Research the codebase for this work item."
- Parameter: `Work ID: workflow-handoffs`

**Why Dynamic Retrieval**:
- **Maintains stage isolation**: Each agent gets fresh context, preventing pollution from prior conversations
- **Supports context updates**: If WorkflowContext.md is updated between stages, agents always read current state
- **Reduces prompt bloat**: Large context (specs, plans) not embedded in every prompt
- **Enables custom instructions**: Workspace and user-level instructions loaded per-agent, per-invocation
- **Single source of truth**: WorkflowContext.md remains authoritative; agents don't carry stale parameters

**Rationale from paw-context tool design** (`.paw/work/paw-context-tool-for-custom-instructions/ImplementationPlan.md:338-362`):
Prior to the context tool, prompt files embedded full paths like:
```markdown
Read WorkflowContext.md at .paw/work/<feature-slug>/WorkflowContext.md
```

This required knowing workspace structure in prompt templates. The tool pattern allows agents to receive just the Work ID and dynamically load all context.

**Evidence**: Context tool implementation `src/tools/contextTool.ts:87-154`; prompt template generation `src/tools/createPromptTemplates.ts:230-250`; agent instructions in `agents/` directory showing `paw_get_context` calls

**Implications**: Handoff tool should pass only Work ID (and optional inline instruction customization) to target agent. Target agent calls `paw_get_context` as first action to retrieve full context. This maintains PAW's stage isolation philosophy while supporting dynamic configuration updates.

---

### Question 4: Optimal Semi-Auto Mode Pause Points for Standard Workflows

**Question**: What are the optimal Semi-Auto mode pause points for standard workflows? Specifically: Should Spec → Spec Research → Spec be fully automated or pause after research input? Should Implementation → Implementation Review → Implementation (next phase) be continuous or pause between phases? Should Documentation generation pause for review before Docs PR creation?

**Answer**: 

Based on workflow analysis from voice notes (`zignore-notes/2025-11-24_workflow-handoff.md`) and specification (`Spec.md`), optimal Semi-Auto pause points balance automation with critical human checkpoints:

**Spec → Spec Research → Spec (AUTOMATED with Research Input Pause)**:
- Spec Agent completes → **auto-start** Spec Research Agent
- Spec Research pauses with message: "I identified these internal research findings. Optional external/context questions are listed for manual fill if desired. Type 'continue' to return to Spec Agent."
- User reviews research, optionally adds external knowledge, then types 'continue'
- **Auto-return** to Spec Agent (no manual command needed)
- Spec Agent finalizes specification → **PAUSE** for user review before proceeding

**Rationale**: Research-Spec loop is tightly coupled—research findings directly inform spec completeness. Automating the return trip reduces friction while the pause for optional external knowledge respects that some questions require human domain expertise.

**Code Research → Implementation Plan (AUTOMATED)**:
- Code Research completes → **auto-start** Implementation Plan Agent
- Implementation Plan finalizes → **PAUSE** for user review

**Rationale**: Code Research naturally flows into planning. Both are pre-implementation analysis stages. Pause after plan allows user to verify approach before costly implementation begins.

**Implementation Plan → Implementation Phase 1 (PAUSE)**:
- Implementation Plan completes → agent presents options: "Ready to implement. Options: 'implement Phase 1', 'customize prompt phase1', 'status'"
- User explicitly commands next step

**Rationale**: Implementation is highest-cost stage. User should confirm readiness and potentially customize instructions per phase.

**Implementation Phase N → Implementation Review → (PAUSE before Phase N+1)**:
- Implementer completes phase → **auto-start** Implementation Review Agent  
- Review Agent completes review, commits changes, pushes branch → **PAUSE** with message: "Phase N complete. Phase PR opened at [link]. Options: 'implement Phase N+1', 'address comments', 'status'"
- User reviews PR, then explicitly commands next phase

**Rationale**: Implementation and Review are tightly coupled—review immediately follows implementation to catch issues early. Pause before next phase allows user to verify PR was merged (if using prs strategy) or confirm phase quality.

**Documentation → Docs PR (AUTOMATED)**:
- Documentation Agent completes Docs.md → **auto-create** docs branch, commit, push, open Docs PR → **PAUSE** with message: "Documentation complete. Docs PR opened at [link]. Options: 'pr' (final PR), 'status'"

**Rationale**: Docs generation is routine; creating the PR is mechanical. Pause after PR creation allows user to review documentation quality before proceeding to final PR.

**Summary of Semi-Auto Pause Points (Standard Workflow)**:
1. **Pause after Spec Research** (for optional external knowledge input)
2. **Pause after Spec finalization** (before Code Research)
3. **Pause after Implementation Plan** (before Phase 1)
4. **Pause after each Phase Review/PR** (before next phase)
5. **Pause after Docs PR** (before Final PR)

**Evidence**: Voice notes describing "thoughtful defaults" where spec-research-spec is automated, code research → implementation plan is automated, but implementation phases pause between; Spec.md defining Semi-Auto mode goals (FR-013)

**Implications**: Semi-Auto agents need conditional logic: check Handoff Mode field in WorkflowContext.md; if "semi-auto", execute auto-handoffs without user command; pause at designated checkpoints with clear next-step options.

---

### Question 5: Optimal Semi-Auto Mode Pause Points for Review Workflows

**Question**: What are the optimal Semi-Auto mode pause points for review workflows? Specifically: Should Understanding → Baseline Research → Understanding be fully automated? Should Impact Analyzer → Gap Analyzer be continuous or pause for impact confirmation? Should Feedback Generator → Feedback Critic iterations be continuous or pause after each iteration?

**Answer**: 

Review workflows follow similar automation patterns to standard workflows, optimized for code review scenarios:

**Understanding → Baseline Research → Understanding (AUTOMATED with Research Review Pause)**:
- Understanding Agent completes initial analysis → **auto-start** Baseline Research Agent
- Baseline Research documents pre-change system behavior → **PAUSE** with message: "I documented baseline behavior. Review findings, then type 'continue' to return to Understanding Agent."
- User reviews baseline (may add context about why changes were made), then types 'continue'
- **Auto-return** to Understanding Agent
- Understanding Agent incorporates baseline findings → **PAUSE** for user to confirm understanding scope

**Rationale**: Parallels Spec → Research → Spec. Understanding and baseline research form a coherent analysis loop. Pause allows user to verify the baseline correctly captures pre-change state before proceeding to impact analysis.

**Understanding → Impact Analyzer (PAUSE)**:
- Understanding completes → agent presents options: "Understanding complete. Ready to analyze impact. Options: 'analyze impact', 'status'"
- User explicitly commands impact analysis start

**Rationale**: Transition from understanding to impact analysis is a significant scope shift—from "what changed?" to "what are the implications?". User should confirm understanding is complete before costly impact analysis.

**Impact Analyzer → Gap Analyzer (AUTOMATED)**:
- Impact Analyzer identifies affected areas → **auto-start** Gap Analyzer
- Gap Analyzer assesses coverage/completeness → **PAUSE** with message: "Gap analysis complete. I identified [X gaps / no gaps]. Options: 'generate feedback', 'status'"

**Rationale**: Impact and gap analysis are sequential analytical stages—impact identifies affected areas, gaps assess coverage. Automating transition reduces friction. Pause before feedback generation allows user to review identified gaps and decide if feedback is warranted.

**Gap Analyzer → Feedback Generator (PAUSE)**:
- User explicitly commands: "generate feedback"

**Rationale**: Feedback generation is an action with external consequences (posting comments to PR). User should confirm gaps warrant feedback before generating.

**Feedback Generator → Feedback Critic (AUTOMATED)**:
- Feedback Generator drafts comments → **auto-start** Feedback Critic
- Feedback Critic reviews/refines comments → **PAUSE** with message: "Feedback draft complete. Review [X comments], then: 'post feedback', 'revise feedback', 'discard'"

**Rationale**: Generation and critique are iterative—critic immediately reviews draft. Automating the handoff maintains flow. Pause before posting ensures user can review and approve feedback content.

**Feedback Critic → Feedback Generator (if revision requested)**:
- User commands: "revise feedback" with instructions
- **Auto-start** Feedback Generator with revision context
- Generator revises → **auto-start** Feedback Critic (iteration continues until user approves or discards)

**Rationale**: Feedback refinement is iterative. Automating Generator ↔ Critic loop allows multiple iterations without manual handoff commands.

**Summary of Semi-Auto Pause Points (Review Workflow)**:
1. **Pause after Baseline Research** (for context review before returning to Understanding)
2. **Pause after Understanding finalization** (before Impact Analysis)
3. **Pause after Gap Analysis** (before Feedback Generation)
4. **Pause after Feedback Critic** (before posting feedback to PR)

**Evidence**: Voice notes mentioning review workflow automation patterns; Spec.md requirements for consistent handoff patterns (FR-012); review workflow stage sequence in paw-specification.md

**Implications**: Review workflow agents need similar conditional logic as standard workflow agents—check Handoff Mode, auto-chain at designated transitions, pause at checkpoints. Feedback loop (Generator ↔ Critic) requires special handling to support multiple iterations with single pause after final critique.

---

### Question 6: Status Agent Directory Scanning for Active Work Items

**Question**: How should the Status Agent efficiently scan `.paw/work/` directory to detect all active work items without performance degradation? What file system patterns indicate "active" vs "abandoned" workflows?

**Answer**: 

The Status Agent should use **shallow directory enumeration** with WorkflowContext.md presence as the primary activity indicator.

**Efficient Scanning Algorithm**:
1. List top-level directories in `.paw/work/` (do not recurse into subdirectories)
2. For each directory, check for `WorkflowContext.md` presence
3. If WorkflowContext.md exists, consider workflow "active" and read it to extract:
   - Work Title
   - Target Branch
   - Review Strategy
   - Handoff Mode (if present)
4. Check git for branch existence: `git rev-parse --verify <target_branch>`
5. Optionally check artifact recency (modification times) to sort by activity

**Performance Characteristics**:
- Reading directory list: O(n) where n = number of work items
- Checking WorkflowContext.md existence: O(n) filesystem stat operations
- Reading WorkflowContext.md: O(n) file reads, but only for active workflows
- No recursion into artifact subdirectories (prompts/, etc.)

**Activity Indicators** (strongest to weakest):
1. **WorkflowContext.md exists**: Primary indicator—file presence means workflow was initialized
2. **Target branch exists in git**: Secondary indicator—branch presence means work started
3. **Recent artifact modifications**: Tertiary indicator—recent changes suggest active work
4. **Artifact completeness**: Phase artifacts (ImplementationPlan.md, Phase commits) indicate progress

**"Abandoned" Workflow Patterns**:
- WorkflowContext.md exists but target branch deleted (user manually cleaned up)
- No artifacts beyond WorkflowContext.md and prompts/ (initialization only, never started)
- All artifacts >30 days old with no git commits on target branch in that period

**Implementation Pattern**:
```typescript
// Pseudocode for Status Agent scanning logic
const workDir = path.join(workspacePath, '.paw', 'work');
const workItems = fs.readdirSync(workDir);

const activeWorkflows = [];
for (const itemDir of workItems) {
  const contextPath = path.join(workDir, itemDir, 'WorkflowContext.md');
  
  // Skip if no WorkflowContext.md (not a valid workflow or abandoned)
  if (!fs.existsSync(contextPath)) continue;
  
  // Read basic context (don't parse full artifacts yet)
  const context = parseWorkflowContext(fs.readFileSync(contextPath));
  
  // Check if branch still exists
  const branchExists = checkGitBranch(context.targetBranch);
  
  activeWorkflows.push({
    workId: itemDir,
    title: context.workTitle,
    branch: context.targetBranch,
    branchExists: branchExists,
    lastModified: fs.statSync(contextPath).mtime
  });
}

// Sort by recency (most recently modified first)
activeWorkflows.sort((a, b) => b.lastModified - a.lastModified);
```

**Caching Strategy** (from Spec.md risk mitigation):
- Cache workflow states with 5-minute TTL
- Cache key: workspace path + work item directory name
- Invalidate cache on filesystem changes to WorkflowContext.md (if VS Code file watcher available)
- For quick status queries, use cached state; for detailed analysis, refresh cache

**Evidence**: Spec.md risk about large work item counts (FR-010); directory structure from paw-specification.md showing `.paw/work/<work-id>/` layout; WorkflowContext.md as authoritative source

**Implications**: Status Agent tool/logic should implement shallow scanning, read only WorkflowContext.md files (not full artifacts) for listing multiple work items. Detailed artifact analysis happens only when user drills into specific workflow. Document recommendation: limit concurrent workflows to ~10 for best performance.

---

### Question 7: Git Commands for Branch Divergence Detection

**Question**: What git commands most reliably detect branch divergence from remote (commits ahead/behind, conflicting changes)? How should the system handle detached HEAD state or branches without remote tracking?

**Answer**: 

Git provides several commands for detecting branch state and divergence:

**Branch Existence and Current Branch**:
```bash
# Check if branch exists locally
git rev-parse --verify <branch-name>  # exit 0 if exists, non-zero if not

# Get current branch name
git branch --show-current  # outputs branch name or empty if detached HEAD

# Check for detached HEAD
git symbolic-ref -q HEAD  # exit 0 if on branch, non-zero if detached HEAD
```

**Detecting Ahead/Behind Status**:
```bash
# Compare local branch to remote
git rev-list --left-right --count origin/<branch>...<branch>

# Output format: "X   Y" where X=behind count, Y=ahead count
# Example: "5   2" means 5 commits behind, 2 commits ahead
```

**Checking Uncommitted Changes**:
```bash
# Check for uncommitted changes (staged or unstaged)
git status --porcelain  # non-empty output means changes exist

# Simpler check for any changes
git diff-index --quiet HEAD --  # exit 0 if clean, non-zero if changes
```

**Checking if Branch Has Remote Tracking**:
```bash
# Get remote tracking branch for current branch
git rev-parse --abbrev-ref --symbolic-full-name @{u}  # outputs e.g., "origin/feature/branch"
# exit non-zero if no upstream configured
```

**Handling Detached HEAD State**:
```bash
# If detached HEAD detected:
# 1. Get the commit SHA
git rev-parse HEAD

# 2. Find branches containing this commit
git branch --contains HEAD

# 3. Inform user they're in detached HEAD and show nearby branches
```

**Handling Branches Without Remote Tracking**:
- Status Agent should check if upstream is configured
- If no upstream: inform user "Branch has no remote tracking. Cannot determine ahead/behind status."
- Suggest: `git push -u origin <branch>` to establish tracking

**Comprehensive Status Check Algorithm**:
```bash
# 1. Check if workspace is a git repo
git rev-parse --is-inside-work-tree

# 2. Get current branch (or detect detached HEAD)
current_branch=$(git branch --show-current)
if [ -z "$current_branch" ]; then
  echo "Detached HEAD state detected"
  exit 1
fi

# 3. Check for uncommitted changes
if ! git diff-index --quiet HEAD --; then
  echo "Uncommitted changes exist"
fi

# 4. Fetch latest from remote (optional but recommended)
git fetch origin --quiet

# 5. Check if upstream is configured
if git rev-parse --abbrev-ref --symbolic-full-name @{u} > /dev/null 2>&1; then
  # 6. Get ahead/behind counts
  counts=$(git rev-list --left-right --count @{u}...HEAD)
  behind=$(echo $counts | cut -f1 -d' ')
  ahead=$(echo $counts | cut -f2 -d' ')
  
  echo "Branch is $behind commits behind and $ahead commits ahead of remote"
else
  echo "No remote tracking branch configured"
fi
```

**Detecting Conflicting Changes**:
```bash
# Check if merge would have conflicts (without actually merging)
git merge-tree $(git merge-base HEAD origin/<branch>) HEAD origin/<branch> | grep -q '<<< HEAD'
# exit 0 if conflicts detected
```

**Evidence**: Git documentation for rev-parse, rev-list, status commands; existing validation in `src/git/validation.ts:1-29` shows pattern of using git commands via child_process

**Implications**: Status Agent should use combination of commands to build comprehensive git state report. For detached HEAD or missing upstream, provide clear guidance rather than failing. Consider adding `--fetch` flag to Status Agent to optionally fetch latest remote state before checking divergence.

---

### Question 8: UX Pattern for Command Keywords in Handoff Recommendations

**Question**: What is the best UX pattern for command keywords in handoff recommendations? Should they be quoted ('research'), code-formatted (`research`), or plain text (research)? How should multi-word commands be formatted?

**Answer**: 

Examining existing PAW agent instructions and patterns reveals **code-formatted with backticks** as the established convention.

**Current Patterns in Agent Instructions**:

From `agents/PAW-01A Specification.agent.md`:
```markdown
- `Spec.md` drafted (written to disk at `.paw/work/<feature-slug>/Spec.md`)
```

From `agents/PAW-03B Impl Reviewer.agent.md`:
```markdown
- [ ] `CodeResearch.md` reviewed and accurate
- [ ] `ImplementationPlan.md` reviewed for completeness
```

From `paw-specification.md`:
```markdown
The **Phased Agent Workflow (PAW)** streamlines development...
**Target Branch**: The branch that will hold all completed work.
```

**Pattern Observations**:
- File names: Always code-formatted (e.g., `Spec.md`, `WorkflowContext.md`)
- Paths: Always code-formatted (e.g., `.paw/work/<slug>/`)
- Technical terms: Code-formatted when referencing code concepts
- Natural language descriptions: Plain text

**Recommended Pattern for Command Keywords**:
- **Single-word commands**: Code-formatted with backticks: `research`, `code`, `implement`, `status`
- **Multi-word commands**: Code-formatted with hyphens or spaces: `implement Phase 2`, `generate prompt`, `address comments`
- **Inline instructions**: Plain text in quotes for user-provided customization: "continue Phase 2 but add rate limiting"

**Example Handoff Message Format**:
```markdown
Specification complete. I identified 5 research questions about the authentication system.

**Next Steps**:
- Type `research` to start Spec Research Agent (auto-returns after research completes)
- Type `code` to skip research and proceed to Code Research
- Type `generate prompt research` to create customizable research prompt file
- Type `status` to see full workflow state
```

**Rationale**:
- **Consistency**: Matches existing PAW documentation style
- **Clarity**: Code formatting visually distinguishes commands from prose
- **Scannability**: Users can quickly identify actionable keywords in agent responses
- **Copyability**: Code-formatted text is easier to copy-paste if needed
- **Familiarity**: Aligns with command-line and technical documentation conventions

**Multi-Word Command Conventions**:
- Use natural spacing for readability: `implement Phase 2` (not `implement-phase-2` or `implementPhase2`)
- Agents parse flexible variations: "implement Phase 2", "implement phase 2", "implement 2" all recognized
- When displaying options, show consistent casing: `implement Phase N`

**Evidence**: Consistent code-formatting in agents/*.agent.md files for technical terms; paw-specification.md formatting conventions

**Implications**: Handoff instructions in agents should format command keywords with backticks. Agent prompts should specify "User may type `keyword` or natural variations; parse flexibly." Documentation and examples should use consistent code-formatted style.

---

### Question 9: Stage Prerequisite Detection and Validation

**Question**: How can the system detect whether stage prerequisites are met (e.g., ImplementationPlan.md exists and is valid before allowing Implementation)? What validation is sufficient vs. overly strict?

**Answer**: 

PAW should use **file existence checks with minimal content validation**—enough to prevent obvious errors without becoming brittle.

**Prerequisite Requirements by Stage**:

| Stage | Required Artifacts | Validation |
|-------|-------------------|------------|
| Spec Research | None (can start immediately) | None |
| Spec (initial) | SpecResearch.md (optional) | Check existence; if missing, proceed without research |
| Code Research | Spec.md | File exists and not empty (>100 bytes) |
| Implementation Plan | Spec.md, CodeResearch.md | Both files exist |
| Implementation Phase 1 | ImplementationPlan.md | File exists and contains "## Phase 1" heading |
| Implementation Phase N | ImplementationPlan.md, Phase N-1 completed | Plan exists; (optionally) check Phase N-1 PR merged |
| Implementation Review | Recent commits on implementation branch | Git log shows commits since last review |
| Documentation | All phases complete | Check for Docs.md absence (indicating not yet generated) |
| Final PR | Docs.md (if docs stage included), all phase PRs merged (if prs strategy) | File existence and PR status |

**Validation Levels**:

**Level 1 - File Existence** (always check):
```javascript
fs.existsSync(path.join(workDir, 'ImplementationPlan.md'))
```

**Level 2 - Non-Empty** (check for critical files):
```javascript
const content = fs.readFileSync(filePath, 'utf-8');
return content.trim().length > 100;  // Arbitrary threshold for "has content"
```

**Level 3 - Structure Validation** (check for specific patterns):
```javascript
// Check ImplementationPlan.md contains Phase headers
const content = fs.readFileSync(planPath, 'utf-8');
return /## Phase \d+/.test(content);  // Contains "## Phase N" heading
```

**Level 4 - Semantic Validation** (too strict, avoid):
❌ Parse full plan to verify phase count matches implementation reality
❌ Validate all sections of Spec.md are complete before allowing Code Research
❌ Check that Implementation commits match planned changes

**Validation Strategy**:
- **Before starting stage**: Check Level 1 (existence) for required artifacts
- **During stage**: Agent validates content as needed for its work
- **Error messages**: Specific and actionable

**Example Error Messages**:
```markdown
Cannot start Implementation Phase 2: ImplementationPlan.md not found.

Please run Implementation Plan Agent first:
1. Type `plan` to create implementation plan
2. Or attach existing plan to chat context
```

```markdown
Cannot start Code Research: Spec.md not found or empty.

Please complete Specification stage first:
1. Type `spec` to create specification
2. Or attach existing Spec.md to chat context
```

**Handling Edge Cases**:
- **Spec.md missing but user insists**: Agent warns, asks for confirmation, proceeds if user confirms
- **ImplementationPlan.md incomplete**: Agent reads plan, identifies missing phases, asks which phase to implement
- **Phase N-1 PR not merged**: Agent checks PR status (if prs strategy), warns user, offers options: "wait for merge", "proceed anyway (local strategy)", "cancel"

**Evidence**: Spec.md requirement FR-015 for prerequisite validation; existing pattern in agents checking for WorkflowContext.md; Implementation Plan structure from paw-specification.md

**Implications**: Agents should check file existence before starting work; provide clear, actionable error messages when prerequisites missing. Avoid deep validation that becomes brittle when users work outside normal flow. Trust agent intelligence to handle incomplete inputs gracefully.

---

### Question 10: Prompt File Naming Convention

**Question**: What prompt file naming convention minimizes conflicts while remaining intuitive? Should phase-specific prompts use "Implementer-Phase3.prompt.md", "03A-impl-phase3.prompt.md", or another pattern?

**Answer**: 

PAW uses a **stage-code prefix system** already established in the codebase. Phase-specific prompts should extend this pattern.

**Existing Naming Convention** (from `src/tools/createPromptTemplates.ts:97-151`):

| Stage | Filename | Agent |
|-------|----------|-------|
| Spec | `01A-spec.prompt.md` | PAW-01A Specification |
| Spec Research | `01B-spec-research.prompt.md` | PAW-01B Spec Researcher |
| Code Research | `02A-code-research.prompt.md` | PAW-02A Code Researcher |
| Implementation Plan | `02B-impl-plan.prompt.md` | PAW-02B Impl Planner |
| Implementation | `03A-implement.prompt.md` | PAW-03A Implementer |
| Implementation Review | `03B-review.prompt.md` | PAW-03B Impl Reviewer |
| PR Review Response | `03C-pr-review.prompt.md` | PAW-03A Implementer |
| PR Review Response Review | `03D-review-pr-review.prompt.md` | PAW-03B Impl Reviewer |
| Documentation | `04-docs.prompt.md` | PAW-04 Documenter |
| Final PR | `05-pr.prompt.md` | PAW-05 PR |
| Status | `0X-status.prompt.md` | PAW-X Status Update |

**Pattern Elements**:
- **Stage code**: `01`-`05` for main stages, `0X` for utility
- **Sub-stage letter**: `A`/`B`/`C`/`D` for variations within a stage
- **Descriptive name**: Kebab-case, concise (e.g., `impl-plan`, `pr-review`)
- **Extension**: Always `.prompt.md`

**Proposed Pattern for Phase-Specific Prompts**:

**Option 1** (Stage-aligned, recommended):
- `03A-implement-phase1.prompt.md`
- `03A-implement-phase2.prompt.md`
- `03A-implement-phase3.prompt.md`
- `03B-review-phase1.prompt.md`
- `03B-review-phase2.prompt.md`

**Option 2** (Agent-aligned):
- `Implementer-Phase1.prompt.md`
- `Implementer-Phase2.prompt.md`
- `Reviewer-Phase1.prompt.md`

**Option 3** (Hybrid):
- `03A-phase1.prompt.md`
- `03A-phase2.prompt.md`

**Recommendation: Option 1** (Stage-aligned with full descriptive names)

**Rationale**:
- **Consistency**: Maintains established stage-code prefix pattern
- **Sortability**: Files sort naturally by stage, then phase
- **Clarity**: Full descriptive name (implement, review) makes purpose obvious
- **Uniqueness**: No conflicts between similarly-named phases across different stages
- **Flexibility**: Can add sub-phase suffixes if needed (e.g., `03A-implement-phase2-auth.prompt.md`)

**Directory Listing Example**:
```
.paw/work/auth-system/prompts/
├── 01A-spec.prompt.md
├── 01B-spec-research.prompt.md
├── 02A-code-research.prompt.md
├── 02B-impl-plan.prompt.md
├── 03A-implement-phase1.prompt.md
├── 03A-implement-phase2.prompt.md
├── 03B-review-phase1.prompt.md
├── 03B-review-phase2.prompt.md
├── 04-docs.prompt.md
├── 05-pr.prompt.md
└── 0X-status.prompt.md
```

**Multi-Phase Combined Prompts** (when combining phases 2-3 into single batch):
- `03A-implement-phase2-3.prompt.md`
- `03B-review-phase2-3.prompt.md`

**Evidence**: Existing prompt templates in `src/tools/createPromptTemplates.ts`; directory structure from `.paw/work/` examples in codebase

**Implications**: Dynamic prompt generation tool should follow this pattern; agent handoff instructions should reference prompts using full filename format; documentation should show examples with consistent naming.

---

### Question 11: Agent Determination from Prompt File Frontmatter

**Question**: How should the handoff tool determine which agent to invoke from prompt file frontmatter? What fallback logic should apply if frontmatter is missing or malformed?

**Answer**: 

The handoff tool should parse the `agent:` field in YAML frontmatter, with fallback to filename-based parsing.

**Frontmatter Format** (established in codebase):
```yaml
---
agent: PAW-02A Code Researcher
---

Research the codebase for this work item.

Work ID: workflow-handoffs
```

**Parsing Algorithm**:

**Step 1**: Extract frontmatter block
```typescript
const frontmatterRegex = /^---\s*\n([\s\S]*?)\n---\s*\n/;
const match = promptContent.match(frontmatterRegex);
if (!match) {
  // No frontmatter found, use fallback
}
```

**Step 2**: Parse YAML frontmatter
```typescript
// Using a simple regex for agent field (avoid full YAML parser dependency)
const agentMatch = match[1].match(/^agent:\s*(.+)$/m);
if (agentMatch) {
  const agentName = agentMatch[1].trim();
  return agentName;  // e.g., "PAW-02A Code Researcher"
}
```

**Step 3**: Fallback to filename parsing
```typescript
// Map filename patterns to agent names
const filenameAgentMap = {
  '01A-spec': 'PAW-01A Specification',
  '01B-spec-research': 'PAW-01B Spec Researcher',
  '02A-code-research': 'PAW-02A Code Researcher',
  '02B-impl-plan': 'PAW-02B Impl Planner',
  '03A-implement': 'PAW-03A Implementer',
  '03B-review': 'PAW-03B Impl Reviewer',
  '04-docs': 'PAW-04 Documenter',
  '05-pr': 'PAW-05 PR',
  '0X-status': 'PAW-X Status Update'
};

// Extract stage code from filename (e.g., "03A-implement-phase2.prompt.md" → "03A-implement")
const basePattern = filename.match(/^(\d+[A-Z]?-[\w-]+)/);
if (basePattern && filenameAgentMap[basePattern[1]]) {
  return filenameAgentMap[basePattern[1]];
}
```

**Step 4**: Error handling if no match
```typescript
throw new Error(
  `Cannot determine agent from prompt file ${filename}. ` +
  `Please ensure frontmatter includes 'agent:' field or filename follows PAW naming convention.`
);
```

**Complete Function** (conceptual):
```typescript
function determineAgentFromPrompt(promptFilePath: string): string {
  const content = fs.readFileSync(promptFilePath, 'utf-8');
  const filename = path.basename(promptFilePath);
  
  // Try frontmatter parsing first
  const agentFromFrontmatter = parseFrontmatterAgent(content);
  if (agentFromFrontmatter) {
    return agentFromFrontmatter;
  }
  
  // Fallback to filename parsing
  const agentFromFilename = parseFilenameAgent(filename);
  if (agentFromFilename) {
    return agentFromFilename;
  }
  
  // No match found
  throw new Error(`Cannot determine agent from ${filename}`);
}
```

**Malformed Frontmatter Handling**:
- **Missing closing `---`**: Treat as no frontmatter, use filename fallback
- **Invalid YAML syntax**: Log warning, use filename fallback
- **Empty agent field**: `agent: ` (no value) - treat as missing, use filename fallback
- **Multiple agent fields**: Use first occurrence

**Evidence**: Frontmatter format in `src/tools/createPromptTemplates.ts:230-250`; existing prompt files in `.paw/work/` directories; agent naming convention in `agents/` directory

**Implications**: Handoff tool should implement robust parsing with fallbacks; generate prompts with valid frontmatter; agents can customize `agent:` field when generating prompts to support variations (e.g., different model preferences).

---

### Question 12: GitHub MCP Tool Methods for PR Status Queries

**Question**: What GitHub MCP tool methods are available for querying PR status? Can we query multiple PRs in a single call, or must we make individual requests per PR? What rate limiting considerations exist?

**Answer**: 

The GitHub MCP Server provides several tools for PR querying, but specific batching capabilities require checking the available MCP tools at runtime.

**Known GitHub MCP Tools** (from semantic search):

1. **`mcp_github_search_pull_requests`**:
   - Search for PRs using GitHub's issues search syntax (pre-scoped to `is:pr`)
   - Parameters: `query`, `sort`, `order`, `page`, `perPage`, `owner`, `repo`
   - Can potentially return multiple PRs in single call via search query
   - Example query: `repo:owner/repo state:open` to find all open PRs

2. **`mcp_github_list_pull_requests`** (inferred from pattern, needs verification):
   - May exist for listing PRs in a repository
   - Likely supports pagination via `page` and `perPage` parameters

3. **Individual PR Queries** (standard GitHub API pattern):
   - Get PR by number: `GET /repos/{owner}/{repo}/pulls/{pull_number}`
   - This is likely wrapped in an MCP tool but specific name unknown from research

**Multi-PR Querying Strategy**:

**Option 1 - Search-Based** (recommended for Status Agent):
```typescript
// Query all PRs for a workflow in single call
const query = `repo:${owner}/${repo} head:${targetBranch} OR head:${targetBranch}_plan OR head:${targetBranch}_phase1`;
const result = await vscode.lm.invokeTool('mcp_github_search_pull_requests', {
  query: query,
  owner: owner,
  repo: repo
});
```

**Option 2 - Sequential Queries** (fallback if search doesn't support complex queries):
```typescript
// Query PRs individually
const planningPR = await queryPR(owner, repo, `${targetBranch}_plan`);
const phase1PR = await queryPR(owner, repo, `${targetBranch}_phase1`);
const phase2PR = await queryPR(owner, repo, `${targetBranch}_phase2`);
```

**Rate Limiting Considerations**:

**GitHub API Rate Limits** (standard for authenticated requests):
- 5,000 requests per hour for authenticated users
- Secondary rate limit: 100 concurrent requests
- GraphQL API has separate limits (point-based system)

**MCP Tool Rate Limiting**:
- MCP tools inherit GitHub API rate limits
- No additional rate limiting at MCP layer (based on available evidence)
- Each tool invocation counts as one API request

**Best Practices**:
- **Batch when possible**: Use search queries to retrieve multiple PRs rather than individual lookups
- **Cache results**: Status Agent should cache PR status for 5-10 minutes to avoid repeated queries during single workflow session
- **Handle rate limit errors**: Catch 403 responses with `X-RateLimit-Remaining: 0` header and inform user
- **Provide `perPage` parameter**: Request reasonable batch sizes (e.g., 30-50) rather than paginating unnecessarily

**Example Rate Limit Error Handling**:
```typescript
try {
  const result = await vscode.lm.invokeTool('mcp_github_search_pull_requests', params);
} catch (error) {
  if (error.statusCode === 403 && error.headers['x-ratelimit-remaining'] === '0') {
    const resetTime = new Date(error.headers['x-ratelimit-reset'] * 1000);
    throw new Error(
      `GitHub API rate limit exceeded. Resets at ${resetTime.toLocaleTimeString()}. ` +
      `Consider reducing Status Agent query frequency.`
    );
  }
  throw error;
}
```

**Evidence**: GitHub MCP tool function declarations from semantic search; GitHub API documentation for rate limits; MCP architecture pattern of wrapping API calls

**Implications**: Status Agent should use search-based queries when possible to minimize API calls. For workflows with many phases (5-10), single search query can retrieve all PRs. Implement caching to avoid repeated queries. Document rate limit handling in error messages.

---

## Open Unknowns

### GitHub MCP Batching Capabilities

**Question**: Can `mcp_github_search_pull_requests` handle complex OR queries to retrieve multiple PRs by different head branches in a single call, or must we make separate queries per branch pattern?

**Rationale**: Research confirmed the tool exists and supports search queries, but the exact syntax for retrieving PRs across multiple branches (`head:branch1 OR head:branch2`) wasn't tested. GitHub's search API supports OR operators in general, but MCP tool behavior needs runtime validation.

**Impact**: If batching is not supported, Status Agent will need to make sequential queries (planning PR, phase PRs, docs PR, final PR), increasing API calls and potential rate limit impact for workflows with many phases.

**Recommendation**: Test during implementation phase; if OR queries fail, implement sequential querying with result caching.

---

### Tool Approval Timeout Duration

**Question**: What is the exact timeout duration for VS Code Language Model Tool approval prompts, and can it be configured by users or extensions?

**Rationale**: Research confirmed timeout exists and is controlled by VS Code/GitHub Copilot, but specific duration (e.g., 30 seconds, 60 seconds) wasn't documented in available sources.

**Impact**: Affects user experience in Auto mode—if timeout is very short (e.g., 10 seconds), users may struggle to approve handoff tools quickly enough. If very long (e.g., 5 minutes), agents may appear "stuck" while waiting.

**Recommendation**: Test during implementation; document observed timeout in user-facing documentation. If timeout is too short for typical workflows, recommend users set "Always Allow" for handoff tool.

---

### Cancellation Token Handling in Handoff Tool

**Question**: When a user cancels a tool invocation (via cancellation token), does VS Code/Copilot provide feedback to the user, or must the tool return a specific error message?

**Rationale**: Research showed context tool checks `token.isCancellationRequested` and returns "Context retrieval was cancelled" message, but whether this message reaches the user or how VS Code handles cancellation wasn't documented.

**Impact**: Affects error handling in handoff tool—if cancellation is handled transparently by VS Code, tool can return simple message. If not, tool needs more elaborate error handling.

**Recommendation**: Follow existing pattern from `paw_get_context` tool (check cancellation, return descriptive message). Test cancellation flow during implementation.

---

## User-Provided External Knowledge (Manual Fill)

The Spec Agent will review these with you. You may provide answers here if possible.

### UX Patterns in VS Code Extensions

1. Are there established UX patterns in VS Code extensions for "workflow wizard" or "guided task completion" that we should align with?

### Industry Best Practices for Agentic Workflows

2. What are industry best practices for "agentic workflow orchestration" in AI-assisted development tools? Are there patterns from Cursor, Windsurf, or other AI IDEs we should consider?

### Research on Automation Levels

3. Are there academic papers or industry research on optimal automation levels in developer workflows that could inform Semi-Auto pause point defaults?

### Multi-Agent System Failure Modes

4. What are common failure modes in multi-agent systems that we should proactively guard against (e.g., infinite loops, context explosion, cascade failures)?

