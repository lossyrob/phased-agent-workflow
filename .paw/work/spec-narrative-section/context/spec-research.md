# The Role of Specifications and Choosing the Right Format

# What Is a Software Specification?

A software specification (or spec) is a detailed description of what a product or feature should do and why, captured in natural language. It serves as a contract or “source of truth” between stakeholders and developers (human or AI) about the expected behavior of the software. Importantly, a functional spec focuses on the product from the user’s perspective, not technical implementation details. As Joel Spolsky famously put it, “A functional specification describes how a product will work entirely from the user’s perspective. It doesn’t care how the thing is implemented.” In other words, a spec captures features, user interactions, and requirements, leaving the how-to-build-it for later in the development process (e. g. in a technical design or plan). In modern AI-assisted development (spec-driven or “agentic” development), the spec becomes even more central. It’s not just a document for humans – it’s also a guide for AI coding agents. One recent definition describes a spec as “a structured, behavior-oriented artifact... written in natural language that expresses software functionality and serves as guidance to AI coding agents.” In this approach, code can be seen as a “compiled” output of the spec. The spec is created (with human input and AI assistance) before coding begins, and the AI uses it as a rulebook to generate an implementation that meets those requirements. This flips the traditional script – instead of code dictating what the product does, the spec dictates what code should do. Therefore, writing a clear spec up front is critical for both human understanding and for AI agents to reliably produce correct code.

# Specifications in the PAW Workflow

In the Phased Agent Workflow (PAW) project, specifications are a key artifact in the first phase of development. The Spec Agent (an AI agent) generates a functional spec document based on a high-level prompt from the human developer. This spec is then used by subsequent agents (e. g. a planning agent, implementing agents) to drive the development. Essentially, PAW follows the same Spec-Driven Development process as GitHub’s Spec Kit: first Specify (produce the spec), then Plan (technical design), then Tasks, then Implement. At each stage, humans review and refine the AI’s output. Currently, PAW’s spec template is derived from GitHub’s Spec Kit. By default, Spec Kit produces a structured Markdown document with sections like User Stories, Functional Requirements, and Success Criteria. For example, after running the spec-generation command, you get a spec. md containing a set of user stories and requirements as defined by the template. These user stories are typically written in an agile format (“As a <user role> , I want <capability> ...”) with accompanying acceptance or success criteria listing what must be true for the feature to be acceptable. The success criteria are essentially the acceptance tests or conditions of satisfaction for each story. This structured format ensures the spec is complete and testable – it enumerates specific behaviors and conditions the system must fulfill.

The challenge: While this structured template enforces thoroughness, it currently lacks an upfront narrative. As noted in PAW’s issue #57, the spec jumps straight into discrete user stories and criteria, making it “difficult to grasp the big picture” . Reviewers find it hard to understand the overall feature or change at a glance, since there’s no summary or contextual story tying the pieces together. This can cause important nuances or cross-story issues to slip past review. In addition, earlier versions of the Spec Agent sometimes included low-level design and code snippets in the spec, which is undesired – the spec should stick to what the system should do, not how to implement it. The PAW team wants to improve the spec format to address these issues.

# Human Readability vs. Machine Interpretability

A good specification must strike a balance between being human-friendly and machine-usable. On one hand, it’s a document for humans – product owners, developers, reviewers – to understand the feature end-to-end and verify it meets the intended goals. On the other hand, in an agentic workflow, the spec is also consumed by an AI planner/coder, which effectively treats it as a formal rulebook or blueprint for implementation. Let’s break down the needs of each:

Human readers (authors/reviewers): Humans benefit from narrative, context, and explanation. A flowing story or overview helps us grok the feature’s purpose and how all the pieces fit together. This is why traditional specs often include an Overview or Background section and even scenario-based narrative. As Joel Spolsky advises, “when you’re designing a product, you need some real-life scenarios for how people will use it… The more vivid and realistic the scenario, the better job you will do designing a product for real users.” In practice, this means an introduction that tells the story of the feature: Who is the user? What problem are we solving? How will the user interact with the system to achieve their goal? A narrative ties together the individual requirements into a coherent picture. This makes it much easier for a human to review the spec and catch when something “feels off” or incomplete. Without a narrative, a list of user stories can feel like disjointed fragments. Indeed, the PAW team “craved” a narrative section because it was too easy to lose the forest for the trees when reading the highly structured output.

AI agents (spec interpreter and planner): LLM-based agents actually handle natural language quite well, but they do best with clarity and structure. The spec serves as input context for planning and coding agents. If it’s well-structured (with consistent headings, lists, and clearly delineated requirements), the AI can more easily parse and refer to specific points. Each user story or requirement can translate into a unit of work (e. g. tasks or code modules) . Structure also reduces ambiguity – for example, having a numbered list of “Functional Requirements” or bullets under “Success Criteria” makes it explicit to the AI what must be satisfied. Too much free-form narrative without structure could risk the AI missing a detail or interpreting something inconsistently. In short, the AI needs the spec to be unambiguous and complete. That said, modern LLMs are capable of understanding context in narrative form too – they benefit from context that explains the why and connects the dots. A well-written narrative can actually improve an AI’s understanding of the intent behind requirements, as long as it’s in addition to (not instead of) explicit requirement listings.

In current practice, many spec-driven development tools favor rigid structure for the AI’s sake. Spec Kit’s default template, for instance, is highly structured with multiple sections and bullet points for each story and criterion. Kiro, another SDD tool, takes a similar approach: it creates a requirements document as a list of user stories each with acceptance criteria (often in a Gherkin-style format like “GIVEN... WHEN... THEN...”) . These structured acceptance tests are essentially human-readable but also serve as clear, testable statements that an AI (or a test suite) can work against. Structure provides consistency, which is useful for automation. However, an overly rigid format can hurt human comprehension. If the spec is just a dry enumeration of points, a human reviewer might struggle to see if the overall solution actually makes sense or if any scenario was forgotten. This is exactly why adding a narrative has been proposed for PAW. Interestingly, community feedback from early adopters of spec-driven workflows supports this. One experienced user noted that making the spec template “ less structured ” and including a freeform narrative section led to better outcomes: “I personally found useful [to] include a reference to a narrative freeform template in addition to the more structured form.” . In other words, combining a narrative overview with the structured list of requirements was more effective than a purely rigid template. The narrative gives the big picture in freeform text, and the structured parts ensure no detail is missed – the best of both worlds.

# Comparing Specification Formats and Styles

Over the history of software development, spec formats have ranged from verbose narrative documents to terse requirement checklists. To understand the best format for our case, it helps to compare a few styles:

Traditional Functional Specs (Waterfall-era): These were often long documents written by product managers or system analysts before any code was written. They tended to read like narrative design documents, describing every feature, screen, and use case in prose, sometimes with mockups or diagrams. They might include sections like Overview, User Personas/Scenarios, Scope and Non-Goals, Functional Requirements, and UI/Workflow Descriptions. The emphasis was on completeness and clarity for humans. For example, Joel Spolsky’s sample specs were written as if telling the story of using the software, with lots of made-up usage details to illustrate each scenario. The advantage was high human readability and shared understanding; the downside was that they could be very lengthy and not directly executable. In today’s terms, a richly narrative spec is great for conveying vision, but an AI agent might need to laboriously extract the specific requirements from the prose if nothing is structured.

Agile User Stories + Acceptance Criteria: In the agile era, big upfront specs were often replaced by user stories in a product backlog. Each user story is a one-sentence narrative (“As a [user], I want [goal], so that [reason]”) and typically has a list of acceptance criteria (“Given... when... then...”). This format is concise and focuses on user-centric slices of functionality. It’s inherently structured (each story is a unit) but also somewhat narrative (written from a user viewpoint). Many AI spec tools essentially generate something akin to a set of user stories with acceptance tests. For instance, Kiro’s “Requirements” step outputs numbered user stories with associated acceptance criteria for each. This style is fairly balanced – it provides context (the user goal in the story) and explicit conditions (the acceptance criteria). A machine can interpret it because the acceptance criteria are explicit, and a human can validate it because the scenario is clear. The PAW spec format already has elements of this: User Stories and Success Criteria (comparable to acceptance tests). What’s missing is an overall narrative glue between the stories.

Structured Templates (Spec Kit and similar): Spec Kit’s template is a bit more formalized than generic agile stories. It includes sections like:

User Stories – a list of all the user-facing scenarios/features.

Functional Requirements – which might break down specific functional needs, possibly derived from the stories (sometimes these overlap or are an elaboration of the stories).

Success Criteria – essentially the acceptance criteria or conditions to meet for success. It may also include other sections (some users add Non-Functional Requirements or Out of Scope sections if needed, although the baseline template might not). The key thing is each section is clearly labeled and formatted with Markdown headers, which both humans and LLMs can navigate easily. For example, Den Delimarsky’s overview of Spec Kit shows “things like user stories, functional requirements, and success criteria” are present in the generated spec. The benefit here is consistency: every spec has the same shape, so an AI agent (or a reviewer) knows exactly where to look for each type of information. The drawback, as we identified, is that it can feel lacking in narrative flow or context up front. It reads more like a formal checklist than a story.

Narrative-Driven Specs (with Freeform Sections): This is the hybrid approach many are now advocating – essentially combining the above. In this style, the spec opens with a Narrative Overview (or Background or Story section) that describes the feature in prose. This section is written almost like a mini product requirement document: it explains the feature’s purpose, the user’s experience of it from start to finish, and any important context or rationale. After this narrative, the spec then provides the structured breakdown (stories, requirements, criteria) to ensure all details are covered. The narrative and structured parts reinforce each other: the narrative ensures understanding, and the structured list ensures thoroughness and testability. This hybrid format seems ideal for PAW’s needs. It’s no coincidence that the PAW team explicitly wants to “start with a strong, detailed narrative section” in the spec – they are moving toward this narrative-driven spec format. Community spec templates are also heading this way; recall the redditor who modified Spec Kit to include a narrative freeform section in addition to the structured template. Essentially, this mirrors how a good product requirements document is written: start with context and high-level story, then list specific requirements.

Formal/Executable Specs and DSLs: At the far end of the spectrum, some approaches (like Tessl, or certain model-driven development tools) treat specs almost like code. They introduce a structured syntax or DSL (domain-specific language) within the spec to make it directly machine-executable. For example, Tessl allows tags like @generate or @test in a spec file to indicate parts that should translate into code or tests. It can even sync spec and code, so that one spec file generates one code module. This is an intriguing approach but is currently experimental. The downside for human readability is that these specs start to look less like natural language and more like a hybrid of code and prose. They are highly structured for the machine’s benefit (even “testable language” for agents), but a non-developer stakeholder might find them hard to read. In the context of PAW, which aims to keep specs in Markdown and understandable by humans, this is probably too rigid. Nonetheless, it’s worth noting that for certain elements of a spec, formal notations are useful – e. g. embedding an API schema in OpenAPI format, or a data model in JSON Schema, can remove ambiguity and let the AI directly know the exact format required. A good spec can include these as sub-sections or attachments for precision, while the main narrative remains human-friendly. For instance, if PAW were specifying an API endpoint, the spec’s narrative might describe the API purpose, but the spec could also include an OpenAPI snippet or interface definition for exact fields (which the AI implementer would then follow). The key is to use formal specs selectively and in service of clarity.

# Best Practices for a Balanced Spec Format

Considering the above, what specification format best balances human readability with AI “rulebook” utility, especially for PAW’s agentic workflow? The evidence and emerging best practices suggest the following

hybrid format is optimal:

1. Begin with a Narrative Overview: Start the spec with a section (e. g. ## Narrative or ## Overview) that tells the story of the feature or change in a cohesive manner. This should be written in normal prose paragraphs. It can describe the feature’s goal, the motivations behind it, and a walkthrough of how an example user would use it. Essentially, it answers: What are we building and why? How will it solve the user’s problem? This section gives context. Reviewers can read this to get an “end-to-end” understanding before diving into details. It should be detailed enough to paint the big picture (not just one sentence), but it doesn’t need to enumerate every requirement – that’s what the next sections are for. For example, a narrative might say: “We are adding a scheduling feature so that a user (teacher) can set up weekly classes. The user will be able to choose dates on a calendar, specify recurrence, and invite students. Once scheduled, notifications are sent...” and so on, describing the flow. This humanizes the spec. It aligns with how product managers write PRDs (Product Requirement Docs) and how classic specs include scenarios. It will also implicitly help the AI by providing a coherent context – the AI will understand the intent behind the subsequent bullet points, reducing the chance of misinterpretation. (In fact, AI models often perform better when given a high-level summary first, then details, as it sets the frame.)

2. Clearly Defined Structured Sections: After the narrative, present the requirements in structured form with consistent headings. The typical sections would be:

User Stories – a list of the specific user scenarios or use-cases the feature must cover. Each story is one high-level capability or interaction. Using the conventional “As a <user>, I want <goal> so that <reason>” format is useful because it ties each requirement to a user value. It’s also easy for both humans and AIs to parse. (Spec Kit already uses this format for stories.) Numbering or bulleting each user story is helpful for reference.

Functional Requirements – a breakdown of the functional needs of the system. In some cases, this might overlap with acceptance criteria or just be another way to list specifics. Some templates combine “User Stories & Requirements” together. The idea here is to list any specific behaviors or rules the system must follow, in a shall/should style. For example: “The system shall send an email notification to all invitees when a class is scheduled.” Each requirement can be tied to one or more user stories. This section ensures no needed functionality is missed even if it wasn’t explicitly described in a single story.

Acceptance Criteria / Success Criteria – a list of conditions that will be used to validate the feature’s success. These are often written per user story (e. g. each story has its own criteria list beneath it). In Spec Kit, “Success Criteria” likely served this role. These criteria are essentially tests that the feature should pass – written in plain language. For instance: “ Given a teacher with no existing classes, when they create a new class event, then it should appear on their calendar immediately.” Using Gherkin syntax (Given/When/Then) is optional but can add clarity by framing each scenario. It’s human-readable (borrowed from Behavior-Driven Development) and unambiguous enough that an AI can interpret it step by step. Whether written in Gherkin or bullet points, the success criteria should cover normal cases and edge cases for each story. Think of them as the acceptance tests in English that the implementation must eventually satisfy.

Non-functional considerations (optional) – if relevant, the spec might also list any constraints like performance requirements, security/privacy requirements, or compliance needs for this feature.(Spec Kit mostly defers technical constraints to the Plan phase, but if something is crucial – e. g. “the app must be accessible under WCAG standards” – it could be noted in the spec as well, so the AI doesn’t ignore it.)

Out of Scope (optional) – sometimes it’s helpful to explicitly mention things that will not be addressed in this feature. This can prevent scope creep and also guide the AI not to wander into unrelated areas. For example, “Out of Scope: scheduling recurring payments (handled by a separate system).” This was a common practice in traditional specs (Joel Spolsky refers to these as “Nongoals”). By telling the AI what not to do, you avoid it over-complicating the plan with extraneous features. Each section should be a Markdown heading (## or ###) so that both the human and AI see a well-organized document. The structured sections essentially form a checklist for both reviewing and later implementation. In fact, Spec Kit introduces a checklist phase where an AI or human can verify that each domain (UX, security, etc.) is covered by the spec. With a structured spec, it’s straightforward to cross-check requirements and success criteria as items to implement or test. An AI planning agent can iterate over each user story or requirement and generate corresponding plan elements and tasks.

3. Keep It Focused on Behavior, Not Design: Ensure the spec (both narrative and lists) describes

what the system should do, not how to code it or design it internally. This is vital for both human and AI. For humans, it keeps the conversation at the right level (stakeholders care about user outcomes, not tech stack choices at spec time). For AI, it prevents confusion and overlap with the planning phase. The spec should not include actual code snippets, data structure definitions, or UX design mockups – those belong in the technical plan or design docs. The PAW issue #57 explicitly wanted to “avoid including code snippets or technical design details in specs” . By keeping the spec purely functional, we maintain a clean separation: the spec is the WHAT/WHY, the plan is the HOW. This also means any technical terms in the spec should be minimal – e. g., say “send an email” rather than specifying an AWS SES service integration. The AI will fill in the technical approach later based on the plan. Maintaining this discipline ensures the spec is maximally reusable – for instance, if you later decided to implement in a different language or architecture, the spec wouldn’t need to change since it’s abstracted from those concerns.

4. Use Consistent Formatting Conventions: Small formatting choices can help with machine interpretability. Use bullet points or numbered lists for enumerations (the AI can then easily count and reference “requirement 3” etc.). Use strong wording like “must”, “should”, “shall” to indicate requirements. Many spec templates also label things like “User Story #1”, “Story 1.1” etc., which can be useful for traceability (the AI’s task list might refer to “implements Story 1.2” for clarity). It’s also helpful to keep language clear and unambiguous. Avoid vagueness like “the system might do X” – if it’s a requirement, state it decisively. For AI, unqualified words like “fast” or “user-friendly” can be unclear; if such non-functional goals are important, try to quantify or specify them (e. g. “load time under 2 seconds for 95% of users” as a success criterion). Essentially, apply good technical writing practices: short sentences, one idea per bullet, and consistent terminology for key entities.

5. Don’t Neglect the Why (Motivations): A great spec format also captures the reasoning behind requirements, at least at a high level. This is usually done in the narrative or by brief rationale notes attached to stories. For example, a user story can have a note “(Rationale: this allows teachers to manage their classes efficiently without using an external calendar tool).” Understanding the why helps both humans and AI make better decisions. If an AI implementer knows the intent, it can choose an implementation that best achieves it (and it’s less likely to do something that technically meets a criterion but misses the spirit). Humans, during review, can likewise challenge a requirement if the rationale doesn’t make sense. In Spec Kit’s philosophy, the spec should capture motivations and context up front. By including a bit of the “why” in the spec (especially in the narrative intro), we ensure it isn’t just a checklist of whats.

# Evolving Spec Formats for AI Coding

The introduction of AI “co-pilots” and autonomous coding agents is indeed changing what a “good spec” looks like. In the past, a spec only had to be read by humans; now it must effectively be understood by an AI as well. That doesn’t mean we must write specs in code or formal logic – natural language works, as long as it’s clear. In fact, maintaining natural language is important because humans are still in the loop as reviewers and stakeholders. That said, spec-driven development has introduced some new practices: for example, maintaining the spec as a living document throughout the project (spec-anchored development) . The spec may be updated as clarifications are made (often with the help of an AI clarify agent) , and as features evolve. This means the spec format should be maintainable over time – another reason to have it well-organized with sections, so modifications can be made in the right place. We also see a trend of specs being more executable: either literally (in frameworks like Tessl) or conceptually (the spec is so thorough that the code is a direct translation). GitHub’s own vision hints that “the lingua franca of development moves to a higher level, and code is the last-mile approach” – implying specs might eventually contain more machine-interpretable content. Even today, some teams mix in formal artifacts (like including an API’s OpenAPI spec as part of the feature spec) to assist AI development. For PAW’s immediate needs, however, the focus should be on human-AI collaboration: use the spec to improve collaboration and understanding, not to rigidly automate everything. The spec is authored by an AI agent but heavily guided and reviewed by a human. Therefore, optimizing the spec for human readability and consistency will yield the best results. A clean narrative + structured format acts as a shared language between the human and the AI. The human can say, “AI, refer to the spec’s narrative for the context on this requirement” or the AI can say (in its analysis), “according to User Story 3 in the spec, we need to implement X” – both can locate and discuss the same parts easily.

# Conclusion: The Ideal Spec Format for PAW

Bringing it all together, the best specification format for PAW’s purposes is one that starts broad and narrows down: begin with an overview narrative to capture the big picture and user perspective, then enumerate specific stories and requirements in a structured way. This format maximizes human understanding and oversight – reviewers get the context they crave – while still providing a rigorous rulebook that an AI agent can systematically follow. It embraces the insight that “a great spec is a narrative that tells the story of the user's problem, and a checklist of what the solution must do” . By incorporating both, we ensure no important detail or assumption is missed (something even automated spec-checklist tools can help verify), and we make the spec enjoyable and logical for humans to read. Concretely, the PAW team should update their spec template (originally from Spec Kit) by adding a

“Narrative” section at the top that describes the feature end-to-end in prose. This narrative should be written in clear paragraphs (3-5 sentences each, for readability) focusing on user experience. Following that, retain the User Stories, Functional Requirements, and Success Criteria sections, but ensure they are well-linked to the narrative (the narrative sets the stage for them). Each user story can be thought of as a slice of the narrative, now formalized. During spec review, the human should verify that the narrative and the listed requirements are consistent (the narrative might even reveal if a requirement was missed or if an extra story is needed). During planning, the AI will use the structured parts to create an implementation plan, but the narrative will help it resolve any ambiguities or prioritize with the user’s perspective in mind. By using this hybrid format, we adhere to proven practices from both classic and AI-driven development. We satisfy the age-old advice to nail down the user experience in writing first, and we leverage the power of structured markdown to guide an AI step-by-step. The spec becomes a living document that is both comprehensive and comprehensible. It tells a coherent story (so you know when things are “off the rails”) and doubles as a precise to-do list for implementation. This balanced approach is what will make the specification an effective tool for PAW’s human developers and AI agents alike.

# References

Den Delimarsky, “What’s The Deal With GitHub Spec Kit” – blog post explaining the purpose of specs in AI development and Spec Kit’s approach. GitHub Spec Kit documentation – describes how Spec Kit templates generate specs with user stories, requirements, success criteria. PAW Issue #57 “Improve Spec Agent Behavior and Template” – outlines the need for a narrative section and removing technical details from specs. Reddit discussion on spec-driven development – user blarg7459 shares lessons about adding a narrative freeform section to spec templates. Joel Spolsky, “Painless Functional Specifications” – classic article series on writing specs from the user perspective (scenarios, no implementation) . Martin Fowler site, “Understanding Spec-Driven Development: Kiro, spec-kit, Tessl” – analysis of spec-driven tools and definitions of specs in the AI context. IntuitionLabs, “GitHub Spec Kit: A Guide to Spec-Driven AI Development” – discusses how specs include user stories, acceptance criteria, flows, and success metrics as a shared truth for AI coding. What's The Deal With GitHub Spec Kit · Den Delimarsky

https://den. dev/blog/github-spec-kit/

GitHub Spec Kit: A Guide to Spec-Driven AI Development | IntuitionLabs

https://intuitionlabs. ai/articles/spec-driven-development-spec-kit

Painless Functional Specifications – Part 2: What’s a Spec? – Joel on Software

https://www. joelonsoftware. com/2000/10/03/painless-functional-specifications-part-2-whats-a-spec/

Understanding Spec-Driven-Development: Kiro, spec-kit, and Tessl

https://martinfowler. com/articles/exploring-gen-ai/sdd-3-tools. html

GitHub - github/spec-kit: Toolkit to help you get started with Spec-Driven Development

https://github. com/github/spec-kit

Improve Spec Agent Behavior and Template

https://github. com/lossyrob/phased-agent-workflow/issues/57

Spec driven development results: r/ChatGPTCoding

https://www. reddit. com/r/ChatGPTCoding/comments/1nl4tcl/spec_driven_development_results/

How to build reliable AI workflows with agentic primitives and context engineering - The GitHub Blog

https://github. blog/ai-and-ml/github-copilot/how-to-build-reliable-ai-workflows-with-agentic-primitives-and-context-engineering/? utm_source=blog-release-oct-2025&utm_campaign=agentic-copilot-cli-launch-2025
