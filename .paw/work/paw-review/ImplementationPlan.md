# PAW Review Workflow Implementation Plan

## Overview
Implement a three-stage review workflow (Understanding → Evaluation → Feedback Generation) producing structured artifacts and enabling evidence-based, human-controlled PR feedback for GitHub and non-GitHub contexts.

## Current State Analysis
Existing PAW workflow implements staged artifact production for specification, planning, implementation, docs, and final PR using chatmode agents, `WorkflowContext.md`, disciplined artifact updates, and GitHub MCP integration. Review workflow artifacts (Spec.md, CodeResearch.md) already define functional/non-functional requirements and patterns to reuse (parameter continuity, artifact chain, pending review flow). Missing: concrete agents, artifact generators for PRContext, DerivedSpec, ChangeAnalysis, ImpactAnalysis, GapAnalysis, ReviewComments with rationale & assessments, non-GitHub branch mode, tone regeneration, question answering capability.

## Desired End State
A set of agent chatmodes and supporting prompts implementing FR-001..FR-034 with artifacts stored under `.paw/reviews/<identifier>/` (PR-N or branch-slug). Reviewer can invoke each stage; artifacts generated in correct dependency order; GitHub pending review (comments only) created; ReviewComments.md contains rationale and assessments; supports regeneration and tone adjustment; no open questions; all success criteria SC-001..SC-016 verifiable.

### Key Discoveries:
- Chatmode pattern & metadata frontmatter (existing agents)
- WorkflowContext parameter extraction pattern
- Research modes (location, analysis, pattern finder) inform CodeResearch reuse
- Pending review creation via `pull_request_review_write` + `add_comment_to_pending_review`
- One Issue, One Comment batching policy in specification

## What We're NOT Doing
- Automatic submission or merging of PRs
- Batch reviews across multiple PRs simultaneously
- Machine learning prioritization or tone personalization profiles
- Extending beyond Must/Should/Could categorization
- Implementing coverage instrumentation tooling (will consume existing metrics only)

## Implementation Approach
Incrementally add new chatmode agent files and shared prompt templates. Reuse existing helper patterns (frontmatter gathering, parameter parsing). Create artifact generator logic inside agents referencing spec requirements; ensure each artifact fully self-contained. Provide internal utility functions (or procedural steps) for: PR metadata fetch, diff/stat retrieval, file categorization, mechanical vs semantic classification, change graph building, baseline code research invocation, impact traversal, test coverage parsing, comment batching, tone regeneration, assessment injection, Q&A referencing artifacts.

## Phase 1: Understanding Stage Agent & Artifacts
### Overview
Implement the Understanding stage producing: `PRContext.md`, `prompts/code-research.prompt.md`, `CodeResearch.md` (generated by separate research agent after pause), `DerivedSpec.md`, `ChangeAnalysis.md`. Establish baseline evidence before evaluation.

### Components / Files
1. Chatmode: `.github/chatmodes/PAW-R1 Understanding Agent.chatmode.md`
2. Artifacts Directory (GitHub PR): `.paw/reviews/PR-<number>/` ; Non-GitHub: `.paw/reviews/<branch-slug>/`
3. Artifacts:
	- `PRContext.md`
	- `prompts/code-research.prompt.md`
	- `CodeResearch.md` (post-research)
	- `DerivedSpec.md`
	- `ChangeAnalysis.md`

### Data Flow
Input: PR URL OR (base branch, current head branch) + `WorkflowContext.md`.
Steps:
1. Fetch PR metadata (GitHub tools) OR derive branch info (git commands) for non-GitHub.
2. Collect changed files (GitHub PR files API; git diff `base...head` in branch mode).
3. Categorize files (implementation/tests/docs/config/generated).
4. Mechanical vs semantic classification.
5. Generate `PRContext.md` with metadata, stats, flags (large PR, mechanical-only, CI status).
6. Generate targeted research questions in `prompts/code-research.prompt.md`.
7. Pause until `CodeResearch.md` appears (external agent run).
8. Create `DerivedSpec.md` using baseline behavior vs diff changes (explicit vs inferred goals, discrepancies, remove open questions prior to Phase 2).
9. Create `ChangeAnalysis.md` summarizing semantic vs mechanical, categories, complexity hotspots, component mapping.

### Algorithms / Heuristics
File Categorization:
```
tests: path contains /test/ or filename matches *_test.*|*.spec.*
docs: extension .md/.rst or path starts docs/
config: extensions (.json,.yml,.yaml,.toml,.ini) or path config/
generated: header comment contains GENERATED or path matches dist/|build/|.paw/autogen/
implementation: default
```
Mechanical vs Semantic:
```
tokenize before/after (lexer or regex for identifiers & keywords)
if logic token delta == 0 AND no new functions/classes AND only whitespace/rename -> mechanical
else -> semantic
```
Complexity hotspot threshold: top 20% of files by semantic token changes OR cyclomatic delta (if tool available) OR >150 changed semantic lines.

Research Question Sections:
1. Baseline behavior for modules with semantic changes
2. Integration points & dependencies (imports/exports/services)
3. Patterns & conventions (error handling, naming, test style)
4. Ambiguities / discrepancies (explicit vs inferred intent)
5. Performance & hot paths (flagged hotspots)
6. Test coverage baseline (changed impl without test updates)

Derived Specification Structure:
```
Intent Summary
Explicit Goals (from PR description/issues)
Inferred Goals (clustered from semantic changes with evidence file:line)
Before Behavior (baseline excerpts referencing CodeResearch file:line)
After Behavior (observed diff patterns file:line)
Changed Interfaces / Data Shapes / APIs
Risks & Invariants
Scope Boundaries / Out-of-Scope
Discrepancies (explicit vs inferred)
Open Questions (MUST be zero before Phase 2)
```

Change Analysis Sections:
```
Summary Stats (files, additions, deletions)
File Categories Table
Mechanical vs Semantic Breakdown
Component Touch Map
Complexity Hot Spots
Integration Points Potentially Affected
Test Change Correlation (impl vs test delta)
```

### Edge Cases
Mechanical-only PR: Skip research prompt & DerivedSpec depth; produce acknowledgment plan (FR-033).
Missing PR description: mark explicit goals empty; add clarification questions.
Large PR (>1000 LOC delta): flag; add narrowed research focusing on high-churn areas (FR-015).
CI failing: note status in `PRContext.md`; avoid redundant research questions on CI-detected style/test failures (FR-032).

### Success Criteria (Automated)
- [ ] `PRContext.md` includes metadata & stats (FR-001..FR-003, FR-032, FR-033 conditional)
- [ ] `prompts/code-research.prompt.md` generated with 6 sections (FR-008)
- [ ] Pause enforced until `CodeResearch.md` exists (FR-009 dependency control)
- [ ] `DerivedSpec.md` distinguishes explicit vs inferred goals & discrepancies (FR-004..FR-006, FR-010, FR-034)
- [ ] `ChangeAnalysis.md` categorizes mechanical vs semantic & file types (FR-002..FR-003)

### Success Criteria (Manual)
- [ ] Reviewer validates DerivedSpec and corrects misinterpretations (FR-007)
- [ ] Zero open questions before proceeding
- [ ] Large PR & mechanical-only flags reviewed

### Contract
Inputs: PR URL or base/head branch; repository clone; WorkflowContext parameters.
Outputs: 5 artifacts ready for Phase 2; zero open questions.
Errors: network/API failures (retry); missing base branch (fetch); absent CodeResearch after timeout (prompt user).

### Implementation Steps
1. Add Understanding Agent chatmode file.
2. Parameter extraction & artifact path resolution.
3. GitHub MCP integration for PR metadata & files.
4. Diff & classification utility functions.
5. Generate `PRContext.md`.
6. Generate research prompt.
7. Wait for `CodeResearch.md`.
8. Produce `DerivedSpec.md` & `ChangeAnalysis.md`.
9. Validate no open questions remain.

### Out-of-Scope Enhancements
AST semantic diff; historical intent mining.

### Assumptions
GitHub API latency low; heuristics adequate without full language parsers.

### Edge Failure Modes
PR API rate limit: backoff; mechanical-only detection fallback to diff stats.
Permission errors: surface and abort phase.

### Verification Commands (illustrative)
Will rely on existing tooling; diff stats via `git diff --numstat base...head` (non-GitHub).

### No Open Questions Guarantee
Agent blocks advancement if `DerivedSpec.md` contains any unresolved questions markers; reviewer must resolve.

## Phase 2: Evaluation Stage Agents & Artifacts
### Overview
Produce `ImpactAnalysis.md` then `GapAnalysis.md` leveraging Phase 1 artifacts (PRContext, CodeResearch, DerivedSpec, ChangeAnalysis). Identify system-wide effects, correctness/safety/security issues, test coverage gaps, performance implications. Provide evidence-backed categorized findings (Must/Should/Could).

### Components / Files
1. Chatmodes:
	 - `.github/chatmodes/PAW-R2 Impact Analysis Agent.chatmode.md`
	 - `.github/chatmodes/PAW-R2 Gap Analysis Agent.chatmode.md`
2. Artifacts Directory: same as Phase 1 (`.paw/reviews/PR-<number>/` or branch-slug)
3. Artifacts:
	 - `ImpactAnalysis.md`
	 - `GapAnalysis.md`

### Data Flow
Input: All Phase 1 artifacts present; no open questions.
Steps:
1. Impact Analysis Agent parses DerivedSpec & ChangeAnalysis for touched components & semantic changes.
2. Build integration graph: follow imports/exports, service registries, API endpoints referenced by changed files.
3. Identify potential breaking changes (modified public function signatures, removed parameters, data shape changes).
4. Assess performance implications: new loops over large collections, added database queries, increased algorithmic complexity (heuristic based on added nested loops, recursion, or large diff additions in hot files).
5. Evaluate security/auth impacts: changed auth middleware, permission checks, cryptographic usage, data validation patterns.
6. Produce `ImpactAnalysis.md` summarizing integration points, breaking changes, performance, security, deployment considerations, dependency updates.
7. Gap Analysis Agent consumes ImpactAnalysis and baseline patterns from CodeResearch to identify issues:
	 - Correctness: logic anomalies, missing edge cases, inconsistent state transitions.
	 - Safety/Security: missing validation, unchecked inputs, broadened permissions.
	 - Compatibility: API changes lacking versioning.
	 - Testing: insufficient test deltas for semantic changes, missing negative/edge case coverage.
	 - Maintainability: duplication introduced, divergence from established patterns.
	 - Performance: inefficient constructs vs baseline.
8. Quantitative test coverage: parse coverage report if available (e.g., `coverage/summary.json` or lcov) else note unavailability.
9. Generate categorized findings with file:line evidence, rationale stub (later expanded in Phase 3), suggestion sketch.

### Algorithms / Heuristics
Integration Graph Construction:
```
for each semantic file:
	parse imports (simple regex per language) -> dependency nodes
	record exported/public symbols changed
	follow one level outward to detect affected consumers (search usages)
```
Breaking Change Detection:
```
if public function signature diff (parameter removed/renamed/type change) OR return type structural change -> record
if config schema keys removed/renamed -> record
if data model fields removed without migration note -> record
```
Performance Heuristics:
```
flag if added nested loops depth >=2 over collections OR added recursion OR increased complexity tokens (if cyclomatic tool) > threshold
flag new large allocation patterns (arrays/maps built from full dataset) without pagination
```
Security/Auth Heuristics:
```
search for auth middleware modifications, permission check removals, new external calls lacking error handling
flag added raw SQL without parameterization, broadened CORS patterns
```
Test Coverage Gap Detection:
```
semantic_impl_files - files referenced in test diffs -> potential gap
check for added branches/conditions lacking branch tests (heuristic: count new 'if','switch','match')
```
Categorization Logic:
```
Must: correctness/safety/security issues with concrete impact evidence
Should: quality/completeness improvements increasing robustness
Could: optional enhancements / style with clear benefit
```

### ImpactAnalysis.md Structure
```
Summary
Integration Points
Breaking Changes
Performance Implications
Security & Authorization Changes
Deployment Considerations
Dependencies & Versioning
Risk Assessment
Scope & Suggested Splitting (if large)
```

### GapAnalysis.md Structure
```
Summary
Findings Table (Must/Should/Could counts)
Sections by Category
	- Correctness
	- Safety/Security
	- Compatibility
	- Testing
	- Maintainability
	- Performance
Each Finding:
	ID, Category, Severity, File:Line, Description, Evidence, Impact/Rationale (stub), Suggestion, Related Findings
Test Coverage Section (quantitative + qualitative)
Scope Assessment
```

### Edge Cases
No semantic changes: empty ImpactAnalysis except acknowledgment; GapAnalysis minimal.
Coverage report missing: mark quantitative metrics unavailable (FR-016 satisfied conditionally).
Oversized PR: scope assessment suggests splitting strategy (FR-015).
Conflicting DerivedSpec vs code: escalate discrepancy in ImpactAnalysis risk section.

### Success Criteria (Automated)
- [ ] `ImpactAnalysis.md` includes integration, breaking, performance, security sections (FR-011..FR-013, FR-015)
- [ ] `GapAnalysis.md` lists findings with file:line evidence (FR-014, FR-016..FR-018)
- [ ] Test coverage section present (FR-016..FR-018) or marks unavailable
- [ ] Must/Should/Could categorization applied (FR-019 prerequisite)

### Success Criteria (Manual)
- [ ] Reviewer validates severity categorization appropriateness
- [ ] Large PR splitting recommendation reviewed
- [ ] Performance and security implications understood & confirmed

### Contract
Input: Phase 1 artifacts
Output: `ImpactAnalysis.md`, `GapAnalysis.md` with evidence-backed findings & coverage assessment
Errors: inability to parse coverage report (fallback), missing baseline artifact (abort).

### Implementation Steps
1. Add Impact Analysis Agent chatmode file.
2. Build integration graph & heuristics utilities.
3. Generate `ImpactAnalysis.md` sections.
4. Add Gap Analysis Agent chatmode file.
5. Implement findings extraction & classification pipeline.
6. Integrate coverage parsing (JSON, lcov, or CLI summary ingestion).
7. Produce `GapAnalysis.md` with structured findings.
8. Validate zero internal tool errors; ensure evidence references valid lines.

### Out-of-Scope Enhancements
Static analysis integration; deep data-flow security analysis.

### Assumptions
Search tools can locate symbol usages; coverage artifacts accessible if tests run.

### Failure Modes
Symbol usage search high false positives (mitigate by file path filtering); insufficient permission for repo (abort).

### Verification Artifacts
Cross-reference counts: number of semantic files in ChangeAnalysis equals integration seed nodes count in ImpactAnalysis.

### No Open Questions Guarantee
GapAnalysis must not include unresolved TBD placeholders; any ambiguity escalated before Phase 3.

## Phase 3: Feedback Generation & Review Comment Assessment
### Overview
Transform GapAnalysis findings into structured review comments, create GitHub pending review (GitHub context) or fully documented `ReviewComments.md` (non-GitHub), enrich each recommendation with Rationale (best practices evidence) and later Assessment (critical evaluation). Provide tone adjustment & regeneration, Q&A capability referencing earlier artifacts, enforce One Issue, One Comment batching.

### Components / Files
1. Chatmodes:
	- `.github/chatmodes/PAW-R3 Feedback Generation Agent.chatmode.md`
	- `.github/chatmodes/PAW-R3 Review Comment Reviewer Agent.chatmode.md`
2. Artifact: `ReviewComments.md`
3. Pending review (GitHub only) via MCP review tools.

### Data Flow
Input: `GapAnalysis.md`, `ImpactAnalysis.md`, `DerivedSpec.md`, `CodeResearch.md`, `PRContext.md` present; zero open questions.
Steps:
1. Feedback Generation Agent ingests findings, groups related issues (same root cause across files) into single comment with multi-location references (One Issue, One Comment).
2. Build comment objects: {id, severity, category, type (inline/thread), file(s), line range(s), description, suggestion, rationale placeholder}.
3. Generate Rationale: cite research notes, baseline patterns from CodeResearch, explicit/inferred goals from DerivedSpec, impact reasoning (correctness/safety/performance etc.).
4. Create `ReviewComments.md` with Summary, Inline Comments, Thread Comments sections; include Rationale for each.
5. GitHub context: create pending review; post inline comments (text + suggestion only), exclude Rationale/Assessment.
6. Reviewer may request tone adjustment: agent deletes pending review, regenerates comments with tone parameters (e.g., encouraging, direct) leaving artifact rationale intact.
7. Review Comment Reviewer Agent reads `ReviewComments.md` and adds Assessment sections (usefulness, accuracy check, alternative perspectives, recommendation include/modify/skip) beneath each comment in markdown; does not modify pending review.
8. Q&A: Feedback Agent answers reviewer queries referencing artifacts; include file:line evidence.

### Comment Generation Details
Severity mapping: Must=blocking corrections; Should=important quality/coverage; Could=optional improvements.
Inline vs Thread Determination:
```
if specific lines changed -> inline
if architectural/theme issue spanning >3 files -> thread
```
Suggestion creation heuristic:
```
If fix is localized -> propose code snippet using existing patterns from CodeResearch.
If broader refactor -> outline approach steps rather than full code.
```

### Rationale Template
```
Rationale:
- Evidence: file:line references
- Baseline Pattern: (from CodeResearch)
- Impact: correctness/safety/security/performance/maintainability
- Best Practice Citation: review-research-notes.md reference
```

### Assessment Template
```
Assessment:
- Usefulness: (high/medium/low + justification)
- Accuracy: validation of evidence references
- Alternative Perspective: other valid interpretation or trade-off
- Recommendation: include / modify / skip
```

### ReviewComments.md Structure (GitHub & Non-GitHub)
```
# Review Comments for PR <number> OR Branch <slug>
Summary Comment
---
## Inline Comments
<comment blocks>
## Thread Comments
<thread blocks>
## Questions for Author (optional)
```

### Tone Adjustment Flow
Parameters: {directness, encouragement_level, formality, conciseness}. Regeneration alters phrasing only; IDs preserved; rationale unchanged.

### Success Criteria (Automated)
- [ ] `ReviewComments.md` generated with all findings (FR-019..FR-024, FR-027, FR-029..FR-031)
- [ ] Pending review created (GitHub) without submission (FR-022, FR-025)
- [ ] Rationale present per comment (FR-027)
- [ ] Assessment sections appended post-reviewer agent run (FR-029, FR-030)
- [ ] Multi-location issues batched (FR-020)
- [ ] Inline vs thread distinction applied (FR-021)

### Success Criteria (Manual)
- [ ] Reviewer edits/deletes comments before submission (FR-025)
- [ ] Tone adjustments reflect requested style (FR-026)
- [ ] Reviewer can trace rationale evidence quickly

### Contract
Inputs: Phase 2 artifacts
Outputs: `ReviewComments.md`, pending review (GitHub), enriched comments
Errors: deletion/regeneration conflicts (resolve by artifact ID preservation), GitHub rate limits (backoff).

### Implementation Steps
1. Add Feedback Generation Agent chatmode file.
2. Implement finding grouping + comment object builder.
3. Generate rationale content referencing research notes & artifacts.
4. Create pending review (GitHub) and post inline comments.
5. Add Review Comment Reviewer Agent chatmode file.
6. Append assessment sections to `ReviewComments.md` only.
7. Implement tone regeneration mechanism.
8. Implement Q&A referencing artifacts (search by keyword, return file:line evidence).
9. Validation: ensure all GapAnalysis findings mapped to at least one comment.

### Out-of-Scope Enhancements
Automated comment prioritization weighting; sentiment analysis.

### Assumptions
GitHub pending review creation remains stable; research notes accessible locally.

### Failure Modes
Reviewer deletes pending review manually before regeneration (recreate from `ReviewComments.md`).
Missing rationale citation (block submission readiness until filled).

### No Open Questions Guarantee
If Assessment identifies informational gaps -> agent must enrich Rationale before allowing final review submission.

## Testing Strategy
### Unit Tests
- WorkflowContext parameter extraction (mock missing fields).
- File categorization & mechanical/semantic heuristics (fixture diffs).
- Integration graph builder (synthetic import trees).
- Breaking change detector (API signature fixtures).
- Finding grouping algorithm (multiple findings same root cause).
- Rationale generator mapping evidence references.
- Tone adjustment preserving IDs.

### Integration Tests
- Phase 1 end-to-end with mock PR API (simulate metadata & changed files).
- Phase 2 end-to-end using generated Phase 1 artifacts.
- Phase 3 end-to-end creating pending review (GitHub mock) and `ReviewComments.md` with rationale & assessment.
- Non-GitHub branch mode end-to-end diff path.

### Manual Verification
1. Large PR flag & splitting recommendation clarity.
2. Regenerated tone does not alter suggestion semantics.
3. Deleted pending review successfully recreated from artifact.
4. Coverage absence annotated properly.
5. Reviewer Q&A returns accurate file:line references.

### Edge Case Tests
- Mechanical-only PR short-circuit.
- Conflicting explicit/inferred goals discrepancy block.
- Missing coverage report fallback.
- Multi-file architectural thread comment creation.

### Test Data Fixtures
Provide synthetic PR diffs with varying sizes; coverage sample JSON; import graphs; baseline CodeResearch excerpts.

### Automation Scripts
Future script `scripts/review-e2e.sh` to orchestrate phases using mocked data for CI regression.

### Acceptance Gate
All automated and manual criteria checklists ticked before enabling agents for production use.

## Performance Considerations
Limit parsing to changed files; one-hop integration expansion. Cache tokenization results. For large diffs >1000 LOC, downgrade to line-level heuristics (skip deep AST). Batch GitHub calls. Stream coverage parsing. Maintain O(n) grouping complexity relative to findings count. Tone regeneration reuses existing parsed tokens, altering text only.

Estimated (non-binding) windows: Phase 1 (5–15 min incl. human research pause), Phase 2 (10–20 min), Phase 3 (10–15 min).

Memory usage proportional to diff size; release intermediate structures post-artifact write.

## Migration Notes
Introduce `.paw/reviews/` directory; no changes to existing `.paw/work/` flows. Backward compatibility maintained; existing agents ignore new review artifacts. Regeneration is idempotent—re-run overwrites only target artifact. Provide script for renaming experimental review artifact paths if any prototypes exist.

Failure recovery: reconstruct pending review from `ReviewComments.md`. Non-GitHub adoption: same algorithms with git diff provider; add manual posting instructions.

Rollback: delete review chatmodes & `.paw/reviews/` directory; revert commits.

Artifact integrity: frontmatter includes commit SHA, date for traceability.

No data migration needed; purely additive.

## Success Criteria Mapping
| Phase | Functional Reqs Covered | Success Criteria |
|-------|-------------------------|------------------|
| 1 (Understanding) | FR-001..FR-010, FR-032, FR-033, FR-034 | SC-001..SC-004, SC-015, SC-016 |
| 2 (Evaluation) | FR-011..FR-018, FR-014, FR-015 | SC-005..SC-006, SC-006, SC-007 (analysis input), SC-006 |
| 3 (Feedback) | FR-019..FR-031, FR-027..FR-030, FR-025..FR-026 | SC-007..SC-013 |

All FR-001..FR-034 mapped; zero gaps.

Open Questions: None.

## Completion Gate
Before publishing planning artifacts:
- All sections complete, no TBD markers.
- Unit/integration test list defined.
- Success criteria mapping table present.
- ImplementationPlan committed alongside Spec & CodeResearch.

## Out-of-Scope Items Confirmation
Confirmed unchanged from earlier section; no new scope creep introduced.

## References
- Spec `.paw/work/paw-review/Spec.md`
- Code Research `.paw/work/paw-review/CodeResearch.md`
- Workflow Context `.paw/work/paw-review/WorkflowContext.md`
- Root specs: `paw-review-specification.md`, `paw-specification.md`
